{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e5cb9274-1851-4c87-86cf-9415d37c71a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "82271da6-023f-4cd8-9b7b-10e857fa8468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3120f-fedc-40c0-a061-3149c50318b6",
   "metadata": {},
   "source": [
    "## Analyze by describing data (initial inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6ef8c7eb-b630-4fa2-95aa-516fedcd4c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "# Which features are available in the dataset?\n",
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ac748e3f-ac6a-4ca7-b07b-2fc69fc12301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fad0b387-471e-4573-b875-d36d7b5b5838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.00</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.00</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                      Name  \\\n",
       "886          887         0       2                     Montvila, Rev. Juozas   \n",
       "887          888         1       1              Graham, Miss. Margaret Edith   \n",
       "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890         1       1                     Behr, Mr. Karl Howell   \n",
       "890          891         0       3                       Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
       "886    male  27.0      0      0      211536  13.00   NaN        S  \n",
       "887  female  19.0      0      0      112053  30.00   B42        S  \n",
       "888  female   NaN      1      2  W./C. 6607  23.45   NaN        S  \n",
       "889    male  26.0      0      0      111369  30.00  C148        C  \n",
       "890    male  32.0      0      0      370376   7.75   NaN        Q  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d3ef8-26b5-492e-8c2d-5307623f7bf3",
   "metadata": {},
   "source": [
    "### Features Data Types\n",
    "* Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n",
    "* Numerical (Continous): Age, Fare. Discrete: SibSp, Parch.\n",
    "* Mixed data types: Ticket (numeric and alphanumeric) Cabin (alphanumeric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "63ce628e-536f-44cc-871e-abd59cb1bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f52aa34f-8c62-4422-b24e-6b6a55ad500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e6f97-f449-40ca-8a11-8a64a07740c0",
   "metadata": {},
   "source": [
    "### Which features contain blank, null or empty values?\n",
    "* Cabin, Age, Embarked features contain a number of null values in that order for the training dataset.\n",
    "* Cabin, Age are incomplete in case of test dataset.\n",
    "* Missing values: Age has only 714 non-null values out of 891, so 177 values are missing. Cabin is mostly missing (204 non-null, meaning 687 missing). Embarked has 889 non-null, so 2 values are missing. In the test set (not shown above), we should also check for missing values (we expect Age and Cabin to have missing values, and possibly Fare might have one missing in test)\n",
    "* These will require correcting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8485f224-91f0-4502-9b1d-772506bdd4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>29.394130</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>1.904602</td>\n",
       "      <td>0.602694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>13.270911</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>1.613459</td>\n",
       "      <td>0.489615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Sex         Age       SibSp       Parch  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838    2.308642    0.647587   29.394130    0.523008    0.381594   \n",
       "std      0.486592    0.836071    0.477990   13.270911    1.102743    0.806057   \n",
       "min      0.000000    1.000000    0.000000    0.420000    0.000000    0.000000   \n",
       "25%      0.000000    2.000000    0.000000   21.000000    0.000000    0.000000   \n",
       "50%      0.000000    3.000000    1.000000   30.000000    0.000000    0.000000   \n",
       "75%      1.000000    3.000000    1.000000   35.000000    1.000000    0.000000   \n",
       "max      1.000000    3.000000    1.000000   80.000000    8.000000    6.000000   \n",
       "\n",
       "             Fare  FamilySize     IsAlone  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean    32.204208    1.904602    0.602694  \n",
       "std     49.693429    1.613459    0.489615  \n",
       "min      0.000000    1.000000    0.000000  \n",
       "25%      7.910400    1.000000    0.000000  \n",
       "50%     14.454200    1.000000    1.000000  \n",
       "75%     31.000000    2.000000    1.000000  \n",
       "max    512.329200   11.000000    1.000000  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the distribution of numerical feature values across the samples?\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f921c3e5-c660-4fee-8220-d78c42eb9a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate: 38.38%\n"
     ]
    }
   ],
   "source": [
    "survival_rate = train_df['Survived'].mean()\n",
    "print(f\"Survival rate: {survival_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "d9a447b5-b08f-4e18-a554-74c934b22a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>29.394130</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>1.904602</td>\n",
       "      <td>0.602694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>13.270911</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>1.613459</td>\n",
       "      <td>0.489615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Sex         Age       SibSp       Parch  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838    2.308642    0.647587   29.394130    0.523008    0.381594   \n",
       "std      0.486592    0.836071    0.477990   13.270911    1.102743    0.806057   \n",
       "min      0.000000    1.000000    0.000000    0.420000    0.000000    0.000000   \n",
       "25%      0.000000    2.000000    0.000000   21.000000    0.000000    0.000000   \n",
       "50%      0.000000    3.000000    1.000000   30.000000    0.000000    0.000000   \n",
       "75%      1.000000    3.000000    1.000000   35.000000    1.000000    0.000000   \n",
       "max      1.000000    3.000000    1.000000   80.000000    8.000000    6.000000   \n",
       "\n",
       "             Fare  FamilySize     IsAlone  \n",
       "count  891.000000  891.000000  891.000000  \n",
       "mean    32.204208    1.904602    0.602694  \n",
       "std     49.693429    1.613459    0.489615  \n",
       "min      0.000000    1.000000    0.000000  \n",
       "25%      7.910400    1.000000    0.000000  \n",
       "50%     14.454200    1.000000    1.000000  \n",
       "75%     31.000000    2.000000    1.000000  \n",
       "max    512.329200   11.000000    1.000000  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b41098-1755-4cf6-b357-272a73307814",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n",
    "- Around 38% samples survived representative of the actual survival rate at 32%.\n",
    "- Fares varied significantly with few passengers (<1%) paying as high as $512."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff918b-8715-43e1-861f-4dcda89dd65a",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering\n",
    "- In this section, we will clean the data and create/modify features to make them more useful for our models. The main steps will be:\n",
    "1. **Handle missing values:**\n",
    "    - Embarked: Fill the two missing Embarked values with the most common port (since only 2 are missing).\n",
    "    - Fare: In the test set, if there's a missing Fare value (we should confirm), fill it with a reasonable value (like median Fare).\n",
    "    - Age: There are quite a few missing ages. We will fill these with a strategy – for example, using the median age for certain groups of passengers (like by Title or Pclass/Sex combination).\n",
    "    - Cabin: This has a lot of missing data. For simplicity, we will drop this feature in our model or perhaps create a new feature like “HasCabin” (1 or 0 indicating if Cabin is known). Cabin might carry some information (e.g., passengers with cabins could be higher class), but since it's mostly missing, a common approach is to drop it.\n",
    "  \n",
    "2. **Feature engineering:**\n",
    "    - Title Extraction: From the Name, extract the title (Mr, Mrs, Miss, Master, etc.) which can be informative (for example, “Master” are young boys, “Miss” and “Mrs” imply female, etc.). We can then reduce rare titles to a generic “Other” category.\n",
    "    - Family Size: Combine SibSp and Parch to create a new feature FamilySize = SibSp + Parch + 1 (the +1 is the passenger themselves). This helps capture whether the passenger was alone or with family. We might also derive IsAlone (a boolean indicating no family aboard, i.e., FamilySize=1).\n",
    "    - Convert categorical to numeric: Convert Sex to a numeric value (e.g., female=0, male=1) and Embarked to numeric. Similarly, convert Title and possibly Pclass if we treat Pclass as categorical.\n",
    "    - Drop unused columns: We can drop Ticket (it’s not obviously useful for prediction) and Name (since we will use Title instead). We will also drop PassengerId from the training set (it’s just an identifier). However, we will keep PassengerId from the test set separately, as it’s needed for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d5d46e4a-16c5-4241-855d-c4638f8d2da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Embarked in train before: 2\n",
      "Missing Embarked in train after: 0\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Embarked\n",
    "# Fill missing Embarked in train set with the mode (most common value)\n",
    "print(\"Missing Embarked in train before:\", train_df['Embarked'].isnull().sum())\n",
    "train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True) # the most frequent value 'S'\n",
    "print(\"Missing Embarked in train after:\", train_df['Embarked'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "fe38e843-8133-40d6-9044-c2601e268b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Fare in test before: 1\n",
      "Missing Fare in test after: 0\n"
     ]
    }
   ],
   "source": [
    "# Handling Missing Fare\n",
    "# Check and fill missing Fare in test set \n",
    "# (Because machine learning models can’t handle missing values like NaN or null during prediction. \n",
    "# If the test.csv file has even one missing value, especially in a column used as a feature (like Fare), \n",
    "# it will throw an error when you try to use the model to make predictions.)\n",
    "print(\"Missing Fare in test before:\", test_df['Fare'].isnull().sum())\n",
    "if test_df['Fare'].isnull().any():\n",
    "    test_df['Fare'].fillna(test_df['Fare'].median(), inplace=True)  # using median\n",
    "print(\"Missing Fare in test after:\", test_df['Fare'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5af1cb41-84aa-43ff-b3b7-c0d8bc4e24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique titles in train: ['Mr' 'Mrs' 'Miss' 'Master' 'Don' 'Rev' 'Dr' 'Mme' 'Ms' 'Major' 'Lady'\n",
      " 'Sir' 'Mlle' 'Col' 'Capt' 'the Countess' 'Jonkheer']\n",
      "Title\n",
      "Mr              517\n",
      "Miss            182\n",
      "Mrs             125\n",
      "Master           40\n",
      "Dr                7\n",
      "Rev               6\n",
      "Mlle              2\n",
      "Major             2\n",
      "Col               2\n",
      "the Countess      1\n",
      "Capt              1\n",
      "Ms                1\n",
      "Sir               1\n",
      "Lady              1\n",
      "Mme               1\n",
      "Don               1\n",
      "Jonkheer          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract Title from Name in both train and test\n",
    "# Extracting the title (Mr, Mrs, Miss, etc.) from the Name can be useful. \n",
    "# For example, it can help us approximate ages for missing values (Master implies child, etc.) \n",
    "# and also serve as an additional categorical feature.\n",
    "import re\n",
    "\n",
    "def extract_title(name):\n",
    "    # Regex: find word ending with a dot following the comma (like \"Mr.\", \"Mrs.\", etc.)\n",
    "    match = re.search(r',\\s*([^\\.]+)\\.', name)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return \"\"\n",
    "\n",
    "# Create Title column\n",
    "train_df['Title'] = train_df['Name'].apply(extract_title)\n",
    "test_df['Title'] = test_df['Name'].apply(extract_title)\n",
    "\n",
    "# Let's see unique titles and their counts in train set\n",
    "print(\"Unique titles in train:\", train_df['Title'].unique())\n",
    "print(train_df['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fee2c5d9-ba7e-4d0a-9c7b-520b5d46a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles after simplification: ['Mr' 'Mrs' 'Miss' 'Master' 'Royalty' 'Officer']\n",
      "Title\n",
      "Mr         517\n",
      "Miss       185\n",
      "Mrs        126\n",
      "Master      40\n",
      "Officer     18\n",
      "Royalty      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We see a variety of titles. The four most common are Mr, Miss, Mrs, Master. \n",
    "# There are some rare ones (Mme, Lady, Sir, etc.) that we can group into an \"Other\" category. \n",
    "# Also note: 'Mlle' and 'Ms' can be considered the same as 'Miss', 'Mme' (French \"Madame\") is similar to 'Mrs'.\n",
    "# We will map these accordingly for simplicity.\n",
    "\n",
    "# Simplify titles by grouping rare titles\n",
    "title_replacements = {\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Ms\": \"Miss\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Lady\": \"Royalty\", \"Countess\": \"Royalty\", \"the Countess\": \"Royalty\",\n",
    "    \"Sir\": \"Royalty\", \"Don\": \"Royalty\", \"Jonkheer\": \"Royalty\",\n",
    "    \"Major\": \"Officer\", \"Col\": \"Officer\", \"Capt\": \"Officer\", \"Rev\": \"Officer\", \"Dr\": \"Officer\"\n",
    "}\n",
    "train_df['Title'] = train_df['Title'].replace(title_replacements)\n",
    "test_df['Title']  = test_df['Title'].replace(title_replacements)\n",
    "\n",
    "# After replacement, check the unique titles\n",
    "print(\"Titles after simplification:\", train_df['Title'].unique())\n",
    "print(train_df['Title'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff00356-bac6-42df-aa6e-0194f77f2227",
   "metadata": {},
   "source": [
    "- Now our Title categories should be more standardized, likely including Mr, Mrs, Miss, Master, Officer, Royalty (where \"Officer\" covers military ranks and clergy, \"Royalty\" covers nobility/honorifics).\n",
    "- This Title feature can be very useful both for missing age imputation and as a predictor itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5d2d959a-b22e-436e-9121-1e024f545e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median ages by Title:\n",
      "Title\n",
      "Master      3.5\n",
      "Miss       21.0\n",
      "Mr         30.0\n",
      "Mrs        35.0\n",
      "Officer    50.0\n",
      "Royalty    40.0\n",
      "Name: Age, dtype: float64\n",
      "Missing Age in train after imputation: 0\n",
      "Missing Age in test after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Imputing Missing Age\n",
    "# Age is missing for many passengers (~20% in train). Instead of dropping those records (which would lose information), we'll fill the missing ages with reasonable guesses. \n",
    "# A common approach is to use median ages of groups.\n",
    "# We can use Title as a grouping key for median age, since titles correlate with age (e.g., Master are young boys, Miss/Mrs are generally younger/older women, etc.). \n",
    "# Let's compute the median age for each Title group and fill accordingly:\n",
    "\n",
    "# Compute median age for each Title group from the training data\n",
    "title_age_median = train_df.groupby('Title')['Age'].median()\n",
    "print(\"Median ages by Title:\")\n",
    "print(title_age_median)\n",
    "\n",
    "# Function to impute age based on Title\n",
    "def impute_age(row):\n",
    "    if pd.isnull(row['Age']):\n",
    "        return title_age_median[row['Title']]\n",
    "    else:\n",
    "        return row['Age']\n",
    "\n",
    "# Apply to both train and test\n",
    "train_df['Age'] = train_df.apply(impute_age, axis=1)\n",
    "test_df['Age']  = test_df.apply(impute_age, axis=1)\n",
    "\n",
    "# Verify no missing Ages remain\n",
    "print(\"Missing Age in train after imputation:\", train_df['Age'].isnull().sum())\n",
    "print(\"Missing Age in test after imputation:\", test_df['Age'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0d94b-deff-4d7a-a757-7bd4f23d0f2b",
   "metadata": {},
   "source": [
    "We group by Title in the training set to get a median age for each title. For example, we might see something like: Master ~4 years, Miss ~21 years, Mr ~30 years, Mrs ~35 years, Officer ~49 years, Royalty ~40 years (just as hypothetical values). These median ages are then used to fill missing Age for passengers with the corresponding Title. After this step, there should be 0 missing ages in both train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0295dada-f75f-4bec-a395-853ce084d3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SibSp  Parch  FamilySize  IsAlone\n",
      "0      1      0           2        0\n",
      "1      1      0           2        0\n",
      "2      0      0           1        1\n",
      "3      1      0           2        0\n",
      "4      0      0           1        1\n"
     ]
    }
   ],
   "source": [
    "# Creating FamilySize and IsAlone\n",
    "# Let's create the FamilySize feature and also an IsAlone indicator:\n",
    "\n",
    "# Create FamilySize feature\n",
    "train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n",
    "test_df['FamilySize']  = test_df['SibSp'] + test_df['Parch'] + 1\n",
    "\n",
    "# Create IsAlone feature (1 if no family on board, i.e., FamilySize == 1)\n",
    "train_df['IsAlone'] = (train_df['FamilySize'] == 1).astype(int)\n",
    "test_df['IsAlone']  = (test_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "print(train_df[['SibSp','Parch','FamilySize','IsAlone']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acc996-95a8-484e-9f0a-9583fabfeb0c",
   "metadata": {},
   "source": [
    "This will show, for example, that Passenger 1 (Braund, Mr. Owen Harris) had SibSp=1, Parch=0, so FamilySize=2 and IsAlone=0 (not alone, he had 1 family member aboard). Passenger 3 (Heikkinen, Miss. Laina) had SibSp=0, Parch=0, FamilySize=1, IsAlone=1 (she was alone). FamilySize can capture the effect of having family: some analyses show that having 1-3 family members could slightly improve survival chances (someone to help), but very large families might reduce chances (harder to get everyone on a lifeboat). The IsAlone feature directly captures if a passenger was solo. We will let the model figure out if these are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1a1db-f5e5-4247-87e2-574240435c33",
   "metadata": {},
   "source": [
    "### Dropping and Converting Columns\n",
    "Now we prepare the final data for modeling:\n",
    "- Drop columns that we don't need in our feature set: PassengerId (from train), Name, Ticket, Cabin (Cabin dropped due to too many missing). We have extracted Title from Name and preserved other useful info, so these can go.\n",
    "- Convert categorical features (Sex, Embarked, Title, and possibly Pclass) to numeric form. We can use one-hot encoding (get dummy variables) or label encoding. For linear models like logistic regression, one-hot encoding is safer to avoid implying an ordinal relationship. For tree-based models, label encoding is fine. Here we'll do one-hot for Embarked and Title for clarity. For Sex, a simple binary mapping is enough. Pclass is ordinal (1,2,3); we can treat it as numeric or one-hot encode it as well. We'll simply leave Pclass as is (or we could one-hot it; either approach works since tree models can handle numeric categories and logistic can interpret numeric categories, but one-hot might yield slightly better logistic performance for non-linear separation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c6ae59fa-7060-4252-91fa-ccb94fc000b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in train set after encoding: ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'Embarked_Q', 'Embarked_S', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer', 'Title_Royalty']\n",
      "Features in test set after encoding: ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone', 'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer']\n"
     ]
    }
   ],
   "source": [
    "# Preserve PassengerId for test set results\n",
    "train_passenger_id = train_df['PassengerId']\n",
    "test_passenger_id = test_df['PassengerId']\n",
    "\n",
    "# Drop columns that won't be used as features\n",
    "drop_cols = ['PassengerId','Name','Ticket','Cabin']\n",
    "train_df.drop(columns=drop_cols, inplace=True)\n",
    "test_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Convert 'Sex' to numeric (female=0, male=1)\n",
    "train_df['Sex'] = train_df['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "test_df['Sex']  = test_df['Sex'].map({'female': 0, 'male': 1}).astype(int)\n",
    "\n",
    "# One-hot encode Embarked and Title\n",
    "train_df = pd.get_dummies(train_df, columns=['Embarked','Title'], drop_first=True)\n",
    "test_df  = pd.get_dummies(test_df,  columns=['Embarked','Title'], drop_first=True)\n",
    "\n",
    "print(\"Features in train set after encoding:\", list(train_df.columns))\n",
    "print(\"Features in test set after encoding:\", list(test_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5c04e-7714-434c-bd53-925c0fbbeab2",
   "metadata": {},
   "source": [
    "We use `pd.get_dummies` with `drop_first=True` to avoid dummy variable trap (for k categories, it creates k-1 dummy columns). After this, the training and test sets might have slightly different dummy columns if a category appeared in train and not in test or vice versa. We should ensure they have the same columns. Commonly, one would concatenate train and test before dummy encoding to ensure consistency. Here, since the titles and embarked values in test should be a subset of those in train (usually), it's likely fine. But to be safe, one could align the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1d7f728e-c8a1-4ea2-920d-f96c6ef323cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Officer</th>\n",
       "      <th>Title_Royalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch      Fare  FamilySize  IsAlone  \\\n",
       "0         3    1  34.5      0      0    7.8292           1        1   \n",
       "1         3    0  47.0      1      0    7.0000           2        0   \n",
       "2         2    1  62.0      0      0    9.6875           1        1   \n",
       "3         3    1  27.0      0      0    8.6625           1        1   \n",
       "4         3    0  22.0      1      1   12.2875           3        0   \n",
       "..      ...  ...   ...    ...    ...       ...         ...      ...   \n",
       "413       3    1  30.0      0      0    8.0500           1        1   \n",
       "414       1    0  39.0      0      0  108.9000           1        1   \n",
       "415       3    1  38.5      0      0    7.2500           1        1   \n",
       "416       3    1  30.0      0      0    8.0500           1        1   \n",
       "417       3    1   3.5      1      1   22.3583           3        0   \n",
       "\n",
       "     Embarked_Q  Embarked_S  Title_Miss  Title_Mr  Title_Mrs  Title_Officer  \\\n",
       "0          True       False       False      True      False          False   \n",
       "1         False        True       False     False       True          False   \n",
       "2          True       False       False      True      False          False   \n",
       "3         False        True       False      True      False          False   \n",
       "4         False        True       False     False       True          False   \n",
       "..          ...         ...         ...       ...        ...            ...   \n",
       "413       False        True       False      True      False          False   \n",
       "414       False       False       False     False      False          False   \n",
       "415       False        True       False      True      False          False   \n",
       "416       False        True       False      True      False          False   \n",
       "417       False       False       False     False      False          False   \n",
       "\n",
       "     Title_Royalty  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  \n",
       "..             ...  \n",
       "413              0  \n",
       "414              0  \n",
       "415              0  \n",
       "416              0  \n",
       "417              0  \n",
       "\n",
       "[418 rows x 15 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align the test set columns with train set columns (add any missing dummy columns in test)\n",
    "for col in train_df.columns:\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = 0\n",
    "# Now ensure test_df has all columns that train_df has (except Survived which is not in test)\n",
    "test_df = test_df[train_df.columns.drop('Survived')]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5842b-d94f-4fd9-a169-ac6a5b7df4b4",
   "metadata": {},
   "source": [
    "## Exploratory Analysis of Key Features\n",
    "Before training models, let's take a quick look at how some features relate to survival, to build our intuition:\n",
    "- Sex: We expect gender to be a strong predictor (the famous \"women and children first\" policy). Let's confirm the survival rates by gender.\n",
    "- Pclass: Socio-economic status might have influenced survival (first class had better access to lifeboats).\n",
    "- FamilySize: Check if being alone vs with family had any effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6565041b-ad53-407c-b5f3-bd0102de069c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survival rate by Sex:\n",
      "Sex\n",
      "0    0.742038\n",
      "1    0.188908\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival rate by Pclass:\n",
      "Pclass\n",
      "1    0.629630\n",
      "2    0.472826\n",
      "3    0.242363\n",
      "Name: Survived, dtype: float64\n",
      "\n",
      "Survival rate by IsAlone (0=not alone, 1=alone):\n",
      "IsAlone\n",
      "0    0.505650\n",
      "1    0.303538\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We'll do a couple of quick calculations:\n",
    "# Survival rates by gender\n",
    "survival_by_sex = train_df.groupby('Sex')['Survived'].mean()\n",
    "print(\"Survival rate by Sex:\")\n",
    "print(survival_by_sex)\n",
    "\n",
    "# Survival rates by Pclass\n",
    "survival_by_class = train_df.groupby('Pclass')['Survived'].mean()\n",
    "print(\"\\nSurvival rate by Pclass:\")\n",
    "print(survival_by_class)\n",
    "\n",
    "# Survival rates by IsAlone\n",
    "survival_by_isalone = train_df.groupby('IsAlone')['Survived'].mean()\n",
    "print(\"\\nSurvival rate by IsAlone (0=not alone, 1=alone):\")\n",
    "print(survival_by_isalone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1cda9-63f9-4714-a03c-ea1e2a5447bb",
   "metadata": {},
   "source": [
    "Interpreting these results (the numbers will be in the output):\n",
    "- By Sex: Suppose it prints Sex=0 (female): ~0.74, Sex=1 (male): ~0.19. That would mean ~74% of females survived vs only ~19% of males. Indeed, in the training set, women had significantly higher survival rate than men, which aligns with expectations​. This confirms that Sex is a very important feature (female passengers were far more likely to survive than males).\n",
    "- By Pclass: It might show Pclass 1: ~0.63, Pclass 2: ~0.47, Pclass 3: ~0.24. First class passengers had around 63% survival, second class ~47%, third class only ~24%​. Clearly being in 1st class greatly improved survival chances, while 3rd class passengers suffered the worst.\n",
    "- By IsAlone: If IsAlone=0 (meaning passenger had family on board), perhaps survival ~0.50; IsAlone=1 (alone) maybe ~0.30 (just approximate). This would suggest passengers with at least one family member had a better survival rate than those alone.\n",
    "  \n",
    "These quick analyses confirm known patterns: female and higher-class passengers survived at higher rates, and having family might have helped somewhat. Our features Sex, Pclass, FamilySize/IsAlone, etc., are capturing these patterns, which our models can leverage. \n",
    "Now we can proceed to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1da8a8-e9f1-4d03-a0a5-7fa0b02fb7aa",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We'll train two models as required:\n",
    "1. Logistic Regression – A simple linear model for classification. This will be our baseline.\n",
    "2. Random Forest – An ensemble of decision trees, a more powerful model that can capture nonlinear relationships.\n",
    "\n",
    "We will train both on the processed training data and evaluate their performance using accuracy. Since Kaggle’s metric for this competition is accuracy (percentage of correct predictions), that will be our focus. We will also be mindful of overfitting: a model that performs extremely well on training data but poorly on unseen data is overfit. We will use a validation split from the training data to check performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e73f9-918d-48cc-bf85-31d7c28a97e6",
   "metadata": {},
   "source": [
    "### Prepare data for modeling\n",
    "- Separate features (X) and target (y) from the training DataFrame. Also, ensure we have the same feature columns in test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7d6340de-6826-4beb-902c-aeb1cc681236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (891, 15)\n",
      "Test features shape: (418, 15)\n",
      "Columns difference: set()\n"
     ]
    }
   ],
   "source": [
    "# Split train_df into X (features) and y (target)\n",
    "X = train_df.drop('Survived', axis=1)\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Ensure X and test_df have the same columns\n",
    "print(\"Train features shape:\", X.shape)\n",
    "print(\"Test features shape:\", test_df.shape)\n",
    "print(\"Columns difference:\", set(X.columns) - set(test_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23720f-f4d8-4044-80ea-4b7eb9ca66e0",
   "metadata": {},
   "source": [
    "### Train/Test Split for validation\n",
    "- We'll hold out a portion of the training data to simulate a validation set (since the test labels are unknown). Let's use 20% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "673609ff-34d4-45f3-be97-b7b988a583fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 712 Validation set size: 179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 80% train, 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0], \"Validation set size:\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90054616-fb3f-48fc-b4e8-2c990561b99c",
   "metadata": {},
   "source": [
    "We use `random_state=42` for reproducibility (Without it, every time you run the code, you get different splits, which means different results). Now we have X_train, y_train for model training, and X_val, y_val for evaluating the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0fa1f0-0375-4c65-b602-ea494df91740",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression\n",
    "- We train a logistic regression model using `sklearn.linear_model.LogisticRegression`. We may need to specify a solver and possibly increase the maximum iterations if it doesn’t converge by default (since we have a moderate number of features after one-hot encoding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "2c4065f6-1cee-4e16-945d-a2c02d4e23d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy on validation set: 0.8156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val_logreg = logreg.predict(X_val)\n",
    "acc_logreg = accuracy_score(y_val, y_pred_val_logreg)\n",
    "print(f\"Logistic Regression accuracy on validation set: {acc_logreg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "641483c5-93a5-44f8-821c-9950d956f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy on training set: 0.8385\n"
     ]
    }
   ],
   "source": [
    "# We can also check the accuracy on the training set to see if the model overfits:\n",
    "# Check training accuracy (to see if there's overfitting)\n",
    "y_pred_train_logreg = logreg.predict(X_train)\n",
    "acc_logreg_train = accuracy_score(y_train, y_pred_train_logreg)\n",
    "print(f\"Logistic Regression accuracy on training set: {acc_logreg_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5aa30a-38fa-4165-82c0-278906960132",
   "metadata": {},
   "source": [
    "Logistic regression is not very flexible, so training accuracy might be similar to validation. If training accuracy and validation accuracy are close, the model is not overfitting badly. If training was much higher, that would be a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a34de8b-4043-4d1b-ab01-db9a09687b98",
   "metadata": {},
   "source": [
    "### Model 2: Random Forest\n",
    "Now train a Random Forest classifier. We'll use sklearn.ensemble.RandomForestClassifier. We can start with default parameters (100 trees, etc.). We will monitor its performance on validation to compare with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2c145761-95f3-46bf-976d-083c96d1859e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy on validation set: 0.8380\n",
      "Random Forest accuracy on training set: 0.9803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred_val_rf = rf_clf.predict(X_val)\n",
    "acc_rf = accuracy_score(y_val, y_pred_val_rf)\n",
    "print(f\"Random Forest accuracy on validation set: {acc_rf:.4f}\")\n",
    "\n",
    "# Training accuracy for comparison\n",
    "y_pred_train_rf = rf_clf.predict(X_train)\n",
    "acc_rf_train = accuracy_score(y_train, y_pred_train_rf)\n",
    "print(f\"Random Forest accuracy on training set: {acc_rf_train:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758e3e3-7066-4417-b4bf-51f376acfadf",
   "metadata": {},
   "source": [
    "The training accuracy for Random Forest could be very high because a forest of deep trees can overfit the training data. For instance, the output might show training accuracy close to 1.0 (i.e., it perfectly classified all training samples). If we see training accuracy much higher than validation (e.g., 1.0 vs 0.83), that indicates some overfitting. However, Random Forest has in-built mechanisms (bagging, feature randomness) to generalize well, so even though it fits training perfectly, it can still perform well on validation. We might consider tuning the Random Forest hyperparameters to mitigate overfitting (like limiting max depth or using fewer features per split), but since our validation accuracy is already decent and not too far from training, it's acceptable. In practice, cross-validation and hyperparameter tuning (grid search) could improve this further, but for now, we are content with these results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d88362-e073-4b05-b082-3c7c45753dc4",
   "metadata": {},
   "source": [
    "In our case, logistic regression had lower variance (underfit a bit, but stable), and Random Forest had higher variance (potential to overfit). The Random Forest’s higher accuracy on validation suggests it's capturing more signal from the data, so we'll likely use it for the final model. \n",
    "\n",
    "Model Selection: We have two models:\n",
    "Logistic Regression accuracy ≈ 81% on validation.\n",
    "Random Forest accuracy ≈ 83% on validation.\n",
    "\n",
    "Random Forest is performing better, so we will choose the Random Forest model for our final predictions on the test set. (If logistic had been close and we preferred simplicity, we might choose logistic, but here the forest wins out.) \n",
    "\n",
    "Before moving on, let's ensure our Random Forest isn't missing any feature that was one-hot encoded differently between train and test. We aligned columns, so it should be fine. \n",
    "\n",
    "But we can also look at feature importance from the Random Forest to see what it found most predictive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9f688d4f-7b4d-48fe-9846-90acd1f18665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 feature importances in Random Forest:\n",
      "Fare        0.250759\n",
      "Age         0.213139\n",
      "Title_Mr    0.124932\n",
      "Sex         0.113282\n",
      "Pclass      0.071523\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Feature importances from Random Forest\n",
    "importances = pd.Series(rf_clf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Top 5 feature importances in Random Forest:\")\n",
    "print(importances.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6b0b3-d6f5-468e-80d2-255a5e9189f9",
   "metadata": {},
   "source": [
    "## Final Prediction on Test Set\n",
    "Now that we have our chosen model (Random Forest), we will train it on the entire training dataset (all 891 samples) to maximize the data used, and then predict on the test dataset of 418 passengers.\n",
    "\n",
    "Why retrain on full train data? Because previously we held out 20% for validation. For the final model, we can use all available training data (since we validated already) to potentially improve the model’s performance a bit with more data. This is a common practice: tune/validate with a split or cross-validation, then train final model on all data for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "321b83c1-f5b9-455e-b691-dbd134fbaa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         1\n",
      "4          896         0\n",
      "5          897         0\n",
      "6          898         0\n",
      "7          899         0\n",
      "8          900         1\n",
      "9          901         0\n"
     ]
    }
   ],
   "source": [
    "# Retrain Random Forest on full training data\n",
    "final_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = final_model.predict(test_df)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_passenger_id,\n",
    "    \"Survived\": test_predictions.astype(int)\n",
    "})\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "60fca6c3-cbeb-432e-b63d-63f2171b8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file saved: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSubmission file saved: submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03d61f-39c9-4ff6-84f5-a8bb52c89f4d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have built a complete pipeline for the Titanic prediction challenge, from data preprocessing to model training and submission. Along the way, we handled missing data, created new features (Title, FamilySize), and saw how certain characteristics (being female, in first class, etc.) strongly affect survival chances. We started with a simple Logistic Regression as a baseline and then moved to a Random Forest which gave us better accuracy. \n",
    "\n",
    "For a beginner, this project illustrates the end-to-end process of a machine learning task:\n",
    "- understanding the data,\n",
    "- cleaning and feature engineering,\n",
    "- trying different models,\n",
    "- avoiding overfitting,\n",
    "- and preparing a submission.\n",
    "\n",
    "With this foundation, you can experiment with more sophisticated techniques to further improve your score. Good luck and happy Kaggleing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd95f0-98b5-4d7b-ba75-5708c0fe1db6",
   "metadata": {},
   "source": [
    "### (Optional) But is this solution the most optimal?\n",
    "- Not yet. It's a great educational baseline, but not leaderboard-crushing. For true optimization, you could\n",
    "    - Use cross-validation instead of a fixed 80/20 split\n",
    "    - Do feature scaling for logistic regression\n",
    "    - Try more powerful models like XGBoost or LightGBM\n",
    "    - Use grid search or RandomizedSearchCV to tune hyperparameters\n",
    "    - Build ensembles (e.g. average predictions from multiple models)\n",
    "    - Do stacking or use feature selection techniques\n",
    "    - Engineer more nuanced features (e.g., Ticket prefixes, Cabin letters, Age buckets, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
